{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_higher_ed.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsMCP76xasnijWWSN5NiEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevin-kaianalytics/coursera_course_reviews/blob/master/bcirp2020_workshop_nlp_higher_ed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRIZhA8OK3bd",
        "colab_type": "text"
      },
      "source": [
        "<p><img alt=\"Kai Analytics logo\" height=\"45px\" src=\"https://static.wixstatic.com/media/8e400c_2cf51ced4ded437f9151bf60ba5a32d0~mv2.png/v1/fill/w_108,h_45,al_c,q_85,usm_0.66_1.00_0.01/8e400c_2cf51ced4ded437f9151bf60ba5a32d0~mv2.webp\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "<br>\n",
        "<br>\n",
        "<h1>Text Analytis for Institutional Research</h1>\n",
        "<h2>BCIRP 2020 Virtual Workshop</h2>\n",
        "\n",
        "The workbook accompanies the following presentation on \n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVyWwWPa5hDs",
        "colab_type": "text"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "The document you are reading is not a static web page, but an interactive environment called **Jupyter Notebook** (*powered by GoLabs*) that lets you write and execute code.\n",
        "\n",
        "For example, below is **code cell** with a short Python script that prints a statement. Simply press the ![Play Button](https://img.icons8.com/ios-glyphs/30/000000/play-button-circled.png) button to see it run!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCn-zTbTL9PF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Hello Everyone!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsACMI14M9nU",
        "colab_type": "text"
      },
      "source": [
        "Woohoo! You ran a Python code! Congratulations. \n",
        "\n",
        "![Thumbs up](https://img.icons8.com/dusk/64/000000/thumb-up.png)\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN0_l6wOR7rH",
        "colab_type": "text"
      },
      "source": [
        "# Load our Dataset\n",
        "\n",
        "Today we will work with a subset of Coursera course feedback dataset from [kaggle](https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset/)\n",
        "\n",
        "Kaggle is the world's largest data science community with powerful tools and resources to help you achieve your data science goals.\n",
        "\n",
        "This dataset contains only feedback related to courses that teach English writing. In the next **code cell**, we will:\n",
        "\n",
        "*   Load the first Python library in today's workshop, PANDAS. It will help us easily read, and manipulate our dateset\n",
        "*   Read our dataset and look at some of its attributes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIg0vTg5aKws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's import PANDAS (Python Data Analysis Library) library.\n",
        "# This one is built into CoLabs, so we don't have to install it.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Set the url to where the dataset is located.\n",
        "# In this case, it's one of my public gitHub repositories.\n",
        "url = 'https://raw.githubusercontent.com/kevin-kaianalytics/coursera_course_reviews/master/writing_course_review.csv'\n",
        "\n",
        "# Read the CSV file.\n",
        "df = pd.read_csv(url) # df is shorthand for dataframe\n",
        "\n",
        "# Let's check and see we've loaded our data properly.\n",
        "# We do this by checking its first 10 rows.\n",
        "pd.options.display.max_columns = None\n",
        "display(df.head(10))\n",
        "\n",
        "# If you want to upload from your desktop\n",
        "# from google.colab import files\n",
        "# df = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ_AtEyCeaMK",
        "colab_type": "text"
      },
      "source": [
        "We can also see how many rows is in our data with the **count()** command. How many rows should we expect to see?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8QtFJcxehKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of rows in the 'Review' column: \", df['Review'].count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bN8m3VGsULN",
        "colab_type": "text"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksWMDfJPi-6_",
        "colab_type": "text"
      },
      "source": [
        "## Our First Word Cloud\n",
        "\n",
        "Okay let's see what a word cloud of the responses look like. *(Don't worry about the complexity of the code here, we're just generating a word cloud)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxMSSWhnjLhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# quickly make all lower case, replace non alpha-numeric characters\n",
        "df_wordcloud = df['Review'].astype(str).str.lower().str.replace('[^\\w\\s]','')\n",
        "\n",
        "text = df_wordcloud.values \n",
        "wordcloud = WordCloud().generate(str(text))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVdap-MVQfHy",
        "colab_type": "text"
      },
      "source": [
        "# Install and Import the NLTK Package\n",
        "\n",
        "\"The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language.\"\n",
        "\n",
        "It is great for for understanding the key concepts we discussed today.\n",
        "\n",
        "In the next **code cell**, you we will be installing and importing the NLTK package.\n",
        "\n",
        "*Note: This may take a moment*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5d-j3GEO1YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install NLTK #Install the Natural Language Toolkit\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mykfo4FVfyaW",
        "colab_type": "text"
      },
      "source": [
        "# Text Pre-Processing\n",
        "\n",
        "In this section, we use some of NLTK text pre-processing functions to help us improve the results of our analysis. As introduced in our presentation, these steps will include:\n",
        "\n",
        "*   Tokenizing text into words\n",
        "*   Removing stop words\n",
        "*   Lemmatization\n",
        "*   Parts of Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biERTpb99OOf",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize the comments\n",
        "\n",
        "From the NLTK Tokenize library, we will import the word_tokenize module.\n",
        "We will also need to download the pre-trained [Punkt tokenizer](https://www.nltk.org/api/nltk.tokenize.html) for English. This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa1XUJiBtHV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import word_tokenize function for the nltk.tokenize library\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the Punkt Sentence Tokenizer from the NLTK Tokenize library\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Some standard text pre-processing here\n",
        "df['Review'] = df['Review'].astype(str).str.lower().str.replace('[^a-zA-Z#]',' ')\n",
        "\n",
        "# Apply Tokenizer and create a new column named \"ReviewTokenized\"\n",
        "df['ReviewTokenized'] = df['Review'].apply(word_tokenize)\n",
        "\n",
        "# Output the first 10 results\n",
        "print(df['ReviewTokenized'].head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRk7DeyNAC9i",
        "colab_type": "text"
      },
      "source": [
        "## Remove Stopwords\n",
        "\n",
        "Remove frequent words that are redundant to our analysis. Let's see what stop words come out of the box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XWIhjF9ALHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download default list of stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download stopwords from the NLTK Tokenize library\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwrRLhK0AfyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will load the English stop words into the stop variable and see it's output\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# Output all the default stopwords\n",
        "print(stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhs5CZdpBrev",
        "colab_type": "text"
      },
      "source": [
        "Let's remove the stop words from our dataset.\n",
        "*This next **code cell** contains some advanced concepts: Lamdas and List Comprehensions. Basically, we apply a one-time function (lambda) that loops through every token in a row and every row, keeping only words not in the stop word list*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6BCdPw0B8S7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['stop_words'] = df['ReviewTokenized'].apply(lambda x: [item for item in x if item not in stop])\n",
        "\n",
        "# Output the first result compare it with the last steps\n",
        "print(df[['Review']].iloc[0].to_string())\n",
        "print('')\n",
        "print(df[['ReviewTokenized']].iloc[0].to_string())\n",
        "print('')\n",
        "print(df[['stop_words']].iloc[0].to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80cLXdRwGLYH",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatize using WordNet's built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet. [WordNet -> Stem](https://www.nltk.org/_modules/nltk/stem/wordnet.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BIGD-vfIJSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Name the WordNetLemmatizer function simply as lemmitizer\n",
        "lemmatizer = nltk.WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYc0eqMaINi2",
        "colab_type": "text"
      },
      "source": [
        "Now we test it with some English words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3z3UsSlHIwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('dogs: ', lemmatizer.lemmatize('dogs'))\n",
        "print('kittens: ', lemmatizer.lemmatize('kittens'))\n",
        "print('students: ', lemmatizer.lemmatize('students'))\n",
        "print('profs: ', lemmatizer.lemmatize('profs'))\n",
        "print('professors: ', lemmatizer.lemmatize('professors'))\n",
        "print('')\n",
        "print('**teaching: ', lemmatizer.lemmatize('teaching'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4euFdNBts9",
        "colab_type": "text"
      },
      "source": [
        "By default, WordNetLemmatizer() treats all words as nouns. So it needs a part of speech tag to help it out.\n",
        "\n",
        "('v' is for verb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRpdQrLrBx_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Now with a verb tag')\n",
        "print('---------')\n",
        "print('teaching: ', lemmatizer.lemmatize('teaching', 'v'))\n",
        "print('teaches: ', lemmatizer.lemmatize('teaches', 'v'))\n",
        "print('taught: ', lemmatizer.lemmatize('taught', 'v'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIDlmVNuQ5OM",
        "colab_type": "text"
      },
      "source": [
        "As you can see, without a part of speech (POS) tag, the WordNetLemmatizer() treats all words as nouns. So how can we assign POS tags more efficiently?\n",
        "\n",
        "We can us the [NLTK Taggers package](https://www.nltk.org/_modules/nltk/tag.html)!\n",
        "This package contains classes and interfaces for part-of-speech tagging, or simply \"tagging\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMEAWBSNRm0u",
        "colab_type": "text"
      },
      "source": [
        "## Parts of Speech Tagging\n",
        "\n",
        "\"This package defines several taggers, which take a list of tokens, assign a tag to each one, and return the resulting list of tagged tokens. Most of the taggers are built automatically based on a training corpus. For example, the unigram tagger tags each word *w* by checking what the most frequent tag for *w* was in a training corpus\" -- *from NLTK 3.5 documentation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YcVD1fCP8M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "df['token_pos'] = df['stop_words'].apply(pos_tag)\n",
        "print(df['token_pos'].head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs2qNQpOeyje",
        "colab_type": "text"
      },
      "source": [
        "### A note on the \"greedy averaged perceptron tagger.\" ###\n",
        "\n",
        "Greedy algorithm is an approach to solving an optimization problem. In the case for this tagger, it picks the most obvious POS representation of a word based on a pre-trained corpus (*Penn Treebank tagset*). There are numerous benefits to this approach but it has an important limitation. It is not uncommon for one English word to hold multiple POS representation. For example: \n",
        "\n",
        "The child had a nasty **fall** from the wall.\n",
        "We went to see a water **fall** yesterday.\n",
        "There is no chance of **fall** in prices of essential commodities.\n",
        "\n",
        "Average means that it that it took an average weight over several iterations during training. Said plainly, it's quite fast, but it's not perfect due to words with more rare POS representations. So use with caution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjGKR1FSFbRE",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Now let's try Lemmitizing again with the additional POS tags. \n",
        "To begin, we need to define a custom function that convert the [Penn Treebank tagset](https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html) to nouns (n), verbs (v) and adjectives (a). These latter three POS are what is supported in NLTK. Actually, if you think about it, prepositions don't have much left in them to truncate! \n",
        "\n",
        "*Sorry, but you won't find any 'But' jokes in this workshop!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbpH4mqmN7lK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize_all(sentence):\n",
        "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
        "        if tag.startswith(\"NN\"):\n",
        "            return lemmatizer.lemmatize(word, pos='n')\n",
        "        elif tag.startswith('VB'):\n",
        "            return lemmatizer.lemmatize(word, pos='v')\n",
        "        elif tag.startswith('JJ'):\n",
        "            return lemmatizer.lemmatize(word, pos='a')\n",
        "        else:\n",
        "            return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xjJwHnJZeUo",
        "colab_type": "text"
      },
      "source": [
        "With the function defined, we can now apply the tagged lemmatization to every row in our dataset. \n",
        "\n",
        "*This might just take a second or two*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nppk5A0kZbcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tagged_token'] = df['ReviewTokenized'].apply(lambda x: [lemmatize_all(y) for y in x]).apply(lambda x: [item for item in x if item not in stop])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0yM6v9DZyku",
        "colab_type": "text"
      },
      "source": [
        "Great! Now we're ready to look at the results. We'll run it just for the first row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHks35EkZdfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "row_number = 0\n",
        "\n",
        "print(df[['Review']].iloc[row_number].to_string())\n",
        "print('')\n",
        "print(df[['ReviewTokenized']].iloc[row_number].to_string())\n",
        "print('')\n",
        "print(df[['stop_words']].iloc[row_number].to_string())\n",
        "print('')\n",
        "print(df[['tagged_token']].iloc[row_number].to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJLyRdbVaJ8c",
        "colab_type": "text"
      },
      "source": [
        "Can you see the difference from the original sentence?\n",
        "\n",
        "Notice how we've simplified our sentence yet retained much of its semantic?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztajvjKeBGrp",
        "colab_type": "text"
      },
      "source": [
        "**[Optional]** If you want, you can download a copy of the results up to this point by running the next **code cell**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWo35reXBGFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "df.to_csv('df.csv')\n",
        "files.download('df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_hdSbGoGU5R",
        "colab_type": "text"
      },
      "source": [
        "# Text Analysis\n",
        "Now that we've applied some common text pre-processing methods (*By the way, congratulations on getting this far*), we can analyze some text! \n",
        "\n",
        "In the next two sections we will take a look at the N-Grams methods nad see what that looks like in terms of visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvVDL_EvA0Ya",
        "colab_type": "text"
      },
      "source": [
        "## N-Grams\n",
        "\n",
        "\"An n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech.\" -- [Wikipedia](https://en.wikipedia.org/wiki/N-gram).\n",
        "\n",
        "Most of the time we're dealing with words, but sometimes we might get to the character level. For example \"A,\" \"T,\" \"G,\" \"C,\" the building blocks of DNA.\n",
        "\n",
        "N-grams can be forward or in reverse. For example, the user may have searched \"blue sky\", but was they in fact interested in the colour, \"sky blue\"?\n",
        "\n",
        "Sometimes you want to skip-gram, especially when it comes to predicting the next word (e.g. autofill). \n",
        "\n",
        "For example, fill in the blank to this sentence, \"the cat ___ on the mat\"...\n",
        "Is it \"sat\", \"played\", or hopefully not... \"pooped\"?\n",
        "\n",
        "---\n",
        "\n",
        "Jokes aside, let's start by importing some libraries to help us with our analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJpOZzB4A4MN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "#Counter will help us keep track of elements (our n-grams) and their count.\n",
        "\n",
        "from nltk.util import ngrams\n",
        "# this is for a generalized n-gram. see also bigrams."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh9w0FThA9VY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify number of grams: bigrams(2), trigrams(3), etc.\n",
        "number_of_grams = 2\n",
        "\n",
        "ngrams_results = []\n",
        "for x in (df['tagged_token']):\n",
        "    ngrams_results.append(list(ngrams(x,number_of_grams))) \n",
        "ngrams_results = [y for x in ngrams_results for y in x]\n",
        "\n",
        "ngram_count = Counter(ngrams_results)\n",
        "\n",
        "dataset_ngram = pd.DataFrame.from_dict(ngram_count, orient='index').reset_index()\n",
        "dataset_ngram.columns = [\"ngrams\", \"count\"]\n",
        "\n",
        "dataset_ngram = dataset_ngram.sort_values(by=['count'], ascending=False).head(40)\n",
        "\n",
        "print(dataset_ngram.sort_values(by=['count'], ascending=False))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t0Ofr8MNp1E",
        "colab_type": "text"
      },
      "source": [
        "## Vizualizing a Network Graph\n",
        "\n",
        "\"In graph theory, a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense \"related\". The objects correspond to mathematical abstractions called vertices and each of the related pairs of vertices is called an edge.\" -- [Wikipedia](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS1HLmqV2DE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = dataset_ngram.set_index('ngrams').T.to_dict('records')\n",
        "n_size = dataset_ngram['count'].astype(int).tolist()\n",
        "n_list = dataset_ngram['ngrams'].astype(str).tolist()\n",
        "\n",
        "print(d)\n",
        "print(len(n_size))\n",
        "print(len(n_list))\n",
        "print(d[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7rlrVuN2X7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files # this will help us save our plot as an image\n",
        "#from matplotlib.pyplot import figure\n",
        "\n",
        "# Create network plot \n",
        "G = nx.Graph()\n",
        "\n",
        "# Create connections between nodes\n",
        "for k, v in d[0].items():\n",
        "    G.add_edge(k[0], k[1], weight=(v * 5))\n",
        "#    G.add_nodes_from(n_list)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8), dpi = 90)\n",
        "\n",
        "pos = nx.spring_layout(G, k=2)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Working on ways to vary the size of each node based on frequency or weight...\n",
        "\n",
        "print(pos)\n",
        "\n",
        "print(G.number_of_nodes())\n",
        "print(G.nodes())\n",
        "print(G.edges())\n",
        "\n",
        "\n",
        "print(len(pos))\n",
        "print(len(G))\n",
        "\n",
        "colors = range(len(G))\n",
        "print(colors)\n",
        "print(type(colors))\n",
        "print(plt.cm.Blues)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#nx.draw(G, pos, node_color=range(24), node_size=800, cmap=plt.cm.Blues)\n",
        "\n",
        "# Plot networks\n",
        "nx.draw_networkx(G, pos,\n",
        "                 font_size=14,\n",
        "#                 nodelist = n_list,\n",
        "#                 node_size= n_size,\n",
        "                 width=3,\n",
        "                 edge_color='grey',\n",
        "                 edge_cmap=plt.cm.Blues,\n",
        "                 node_color='#4CB1D7',\n",
        "                 with_labels = False,\n",
        "                 ax=ax\n",
        "                 )\n",
        "\n",
        "# nx.draw_networkx_edges(G = G, pos = pos, edge_color='#73CAC7', alpha=0.6, width=(n_size))\n",
        "\n",
        "# Create offset labels to improve readability\n",
        "for key, value in pos.items():\n",
        "    x, y = value[0], value[1]\n",
        "    ax.text(x, y,\n",
        "            s=key,\n",
        "            bbox=dict(facecolor='#5DC5C1', alpha=0.25),\n",
        "            horizontalalignment='center', fontsize=12)\n",
        "\n",
        "# If you want to download the graph, just uncomment the next two lines\n",
        "# plt.savefig(\"network_graph_coursera.png\")\n",
        "# files.download(\"network_graph_coursera.png\")    \n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1TezVyN1yTu",
        "colab_type": "text"
      },
      "source": [
        "## And Finally, A Better Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THvgVZ3b147S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wordcloud as w\n",
        "\n",
        "word_dict = dict(zip(dataset_ngram[\"ngrams\"].astype(str), dataset_ngram[\"count\"]))\n",
        "# word cloud needs a dictionary format if we're generating from frequencies\n",
        "\n",
        "wordcloud = w.WordCloud(collocations=False, \n",
        "                        max_words=500, \n",
        "                        width=1000,\n",
        "                        height=500).generate_from_frequencies(word_dict)\n",
        "\n",
        "# Set plot size and output the graph\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# If you want to download the word cloud\n",
        "# plt.savefig(\"ngram_coursera_wordcloud.png\")\n",
        "# files.download(\"ngram_coursera_wordcloud.png\")    \n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}